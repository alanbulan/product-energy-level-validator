"""
å¿«é€Ÿæ‰¹é‡å¤„ç†å™¨
ä¸“æ³¨äºé«˜æ•ˆã€ç¨³å®šåœ°ä»ç½‘ç«™è·å–æ•°æ®
"""

import time
import random
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from queue import Queue
from typing import List, Dict, Any, Tuple
from anti_crawler import AntiCrawlerHandler
from model_extractor import ModelExtractor
from result_comparator import ResultComparator


class FastBatchProcessor:
    def __init__(self, max_workers: int = 3, request_interval: float = 2.0):
        """
        åˆå§‹åŒ–å¿«é€Ÿæ‰¹é‡å¤„ç†å™¨
        
        Args:
            max_workers: æœ€å¤§å¹¶å‘å·¥ä½œçº¿ç¨‹æ•°
            request_interval: è¯·æ±‚é—´éš”æ—¶é—´ï¼ˆç§’ï¼‰
        """
        self.max_workers = max_workers
        self.request_interval = request_interval
        self.extractor = ModelExtractor()
        self.comparator = ResultComparator()
        
        # çº¿ç¨‹å®‰å…¨çš„ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_processed': 0,
            'successful': 0,
            'failed': 0,
            'rate_limited': 0,
            'start_time': None
        }
        self.stats_lock = threading.Lock()
        
        # è¯·æ±‚é¢‘ç‡æ§åˆ¶
        self.last_request_time = {}
        self.request_lock = threading.Lock()
        
        print(f"ğŸš€ å¿«é€Ÿæ‰¹é‡å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ (å·¥ä½œçº¿ç¨‹: {max_workers}, é—´éš”: {request_interval}s)")
    
    def _get_crawler_for_thread(self) -> AntiCrawlerHandler:
        """ä¸ºæ¯ä¸ªçº¿ç¨‹è·å–ç‹¬ç«‹çš„çˆ¬è™«å®ä¾‹"""
        thread_id = threading.current_thread().ident
        if not hasattr(self, '_crawlers'):
            self._crawlers = {}
        
        if thread_id not in self._crawlers:
            self._crawlers[thread_id] = AntiCrawlerHandler()
            print(f"ğŸ•·ï¸ ä¸ºçº¿ç¨‹ {thread_id} åˆ›å»ºçˆ¬è™«å®ä¾‹")
        
        return self._crawlers[thread_id]
    
    def _rate_limit_control(self):
        """è¯·æ±‚é¢‘ç‡æ§åˆ¶"""
        with self.request_lock:
            thread_id = threading.current_thread().ident
            current_time = time.time()
            
            if thread_id in self.last_request_time:
                elapsed = current_time - self.last_request_time[thread_id]
                if elapsed < self.request_interval:
                    sleep_time = self.request_interval - elapsed
                    print(f"â° çº¿ç¨‹ {thread_id} ç­‰å¾… {sleep_time:.1f}s")
                    time.sleep(sleep_time)
            
            self.last_request_time[thread_id] = time.time()
    
    def _update_stats(self, success: bool, rate_limited: bool = False):
        """æ›´æ–°ç»Ÿè®¡ä¿¡æ¯"""
        with self.stats_lock:
            self.stats['total_processed'] += 1
            if success:
                self.stats['successful'] += 1
            else:
                self.stats['failed'] += 1
            if rate_limited:
                self.stats['rate_limited'] += 1
    
    def _search_single_product(self, model_data: Tuple[int, str, str]) -> Dict[str, Any]:
        """
        æœç´¢å•ä¸ªäº§å“ï¼ˆçº¿ç¨‹å®‰å…¨ç‰ˆæœ¬ï¼‰
        
        Args:
            model_data: (row_index, model, excel_energy_level)
            
        Returns:
            Dict: æœç´¢å’ŒéªŒè¯ç»“æœ
        """
        row_index, original_model, excel_energy_level = model_data
        thread_id = threading.current_thread().ident
        
        result = {
            'row_index': row_index,
            'original_model': original_model,
            'excel_energy_level': excel_energy_level,
            'search_success': False,
            'validation_result': 'æœä¸åˆ°',
            'details': '',
            'thread_id': thread_id
        }
        
        try:
            # é¢‘ç‡æ§åˆ¶
            self._rate_limit_control()
            
            # è·å–çº¿ç¨‹ä¸“ç”¨çš„çˆ¬è™«å®ä¾‹
            crawler = self._get_crawler_for_thread()
            
            # æå–å‹å·
            extracted_model = self.extractor.extract_model(original_model)
            
            # æœç´¢äº§å“
            search_result = crawler.search_product(extracted_model, max_retries=2)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å®é™…çš„è®°å½•è¿”å›ï¼ˆå¿½ç•¥é”™è¯¯çš„totalå­—æ®µï¼‰
            records = search_result.get('list', []) if search_result else []
            if search_result and len(records) > 0:
                result['search_success'] = True
                
                # æå–èƒ½æ•ˆç­‰çº§ä¿¡æ¯
                energy_levels = crawler.extract_energy_levels(search_result)
                
                if energy_levels:
                    # æ‰¾åˆ°æœ€ä½³åŒ¹é…
                    mock_search_result = {
                        'search_success': True,
                        'energy_levels': energy_levels,
                        'extracted_model': extracted_model
                    }
                    
                    best_match = self.comparator.find_best_match(mock_search_result, original_model)
                    
                    if best_match:
                        search_energy_level = best_match['energy_level']
                        
                        # å¯¹æ¯”éªŒè¯
                        validation_result = self.comparator.compare_energy_levels(
                            excel_energy_level, search_energy_level
                        )
                        
                        result['validation_result'] = validation_result
                        result['search_energy_level'] = search_energy_level
                        result['matched_model'] = best_match['model']
                        result['producer'] = best_match.get('producer', '')
                        
                        # ç”Ÿæˆè¯¦ç»†ä¿¡æ¯
                        if validation_result == 'æ­£ç¡®':
                            result['details'] = f"åŒ¹é…: {best_match['model']} - {search_energy_level}"
                        elif validation_result == 'é”™è¯¯':
                            result['details'] = f"ä¸åŒ¹é…: Excel({excel_energy_level}) vs æœç´¢({search_energy_level})"
                        elif validation_result == 'Excelç¼ºå¤±':
                            result['details'] = f"Excelç¼ºå¤±ï¼Œç½‘ç«™æ˜¾ç¤º: {search_energy_level} - {best_match['model']}"
                        else:
                            result['details'] = f"æ— æ³•ç¡®å®š: {best_match['model']}"
                    else:
                        result['details'] = f"åœ¨ {len(energy_levels)} æ¡è®°å½•ä¸­æœªæ‰¾åˆ°åŒ¹é…"
                else:
                    result['details'] = "æ— æ³•æå–èƒ½æ•ˆç­‰çº§ä¿¡æ¯"
            else:
                result['details'] = "æœç´¢æ— ç»“æœ"
            
            # æ›´æ–°ç»Ÿè®¡
            self._update_stats(result['search_success'])
            
        except Exception as e:
            result['details'] = f"å¤„ç†å‡ºé”™: {str(e)}"
            self._update_stats(False)
            print(f"âŒ çº¿ç¨‹ {thread_id} å¤„ç† {original_model} æ—¶å‡ºé”™: {str(e)}")
        
        return result
    
    def process_batch(self, products_data: List[Tuple[int, str, str]]) -> List[Dict[str, Any]]:
        """
        æ‰¹é‡å¤„ç†äº§å“æ•°æ®
        
        Args:
            products_data: [(row_index, model, excel_energy_level), ...]
            
        Returns:
            List[Dict]: å¤„ç†ç»“æœåˆ—è¡¨
        """
        print(f"ğŸš€ å¼€å§‹æ‰¹é‡å¤„ç† {len(products_data)} ä¸ªäº§å“")
        print(f"âš™ï¸ é…ç½®: {self.max_workers} ä¸ªå·¥ä½œçº¿ç¨‹, {self.request_interval}s é—´éš”")
        
        self.stats['start_time'] = time.time()
        results = []
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤„ç†
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_data = {
                executor.submit(self._search_single_product, data): data 
                for data in products_data
            }
            
            # æ”¶é›†ç»“æœ
            for i, future in enumerate(as_completed(future_to_data), 1):
                try:
                    result = future.result()
                    results.append(result)
                    
                    # æ˜¾ç¤ºè¿›åº¦
                    if i % 10 == 0 or i == len(products_data):
                        self._print_progress(i, len(products_data))
                        
                except Exception as e:
                    data = future_to_data[future]
                    print(f"âŒ ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {data[1]} - {str(e)}")
                    
                    # åˆ›å»ºå¤±è´¥ç»“æœ
                    error_result = {
                        'row_index': data[0],
                        'original_model': data[1],
                        'excel_energy_level': data[2],
                        'search_success': False,
                        'validation_result': 'æœä¸åˆ°',
                        'details': f'ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {str(e)}'
                    }
                    results.append(error_result)
        
        # æŒ‰è¡Œç´¢å¼•æ’åºç»“æœ
        results.sort(key=lambda x: x['row_index'])
        
        # æ‰“å°æœ€ç»ˆç»Ÿè®¡
        self._print_final_stats()
        
        return results
    
    def _print_progress(self, current: int, total: int):
        """æ‰“å°è¿›åº¦ä¿¡æ¯"""
        with self.stats_lock:
            elapsed = time.time() - self.stats['start_time']
            rate = current / elapsed if elapsed > 0 else 0
            eta = (total - current) / rate if rate > 0 else 0
            
            print(f"ğŸ“Š è¿›åº¦: {current}/{total} ({current/total*100:.1f}%) | "
                  f"æˆåŠŸ: {self.stats['successful']} | "
                  f"å¤±è´¥: {self.stats['failed']} | "
                  f"é€Ÿåº¦: {rate:.1f}/s | "
                  f"é¢„è®¡å‰©ä½™: {eta/60:.1f}åˆ†é’Ÿ")
    
    def _print_final_stats(self):
        """æ‰“å°æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯"""
        with self.stats_lock:
            elapsed = time.time() - self.stats['start_time']
            total = self.stats['total_processed']
            successful = self.stats['successful']
            failed = self.stats['failed']
            rate_limited = self.stats['rate_limited']
            
            print(f"\nğŸ‰ æ‰¹é‡å¤„ç†å®Œæˆ!")
            print(f"ğŸ“Š æœ€ç»ˆç»Ÿè®¡:")
            print(f"  æ€»å¤„ç†: {total}")
            print(f"  æˆåŠŸ: {successful} ({successful/total*100:.1f}%)")
            print(f"  å¤±è´¥: {failed} ({failed/total*100:.1f}%)")
            print(f"  é¢‘ç‡é™åˆ¶: {rate_limited}")
            print(f"  æ€»è€—æ—¶: {elapsed/60:.1f} åˆ†é’Ÿ")
            print(f"  å¹³å‡é€Ÿåº¦: {total/elapsed:.1f} ä¸ª/ç§’")
    
    def close(self):
        """å…³é—­æ‰€æœ‰çˆ¬è™«å®ä¾‹"""
        if hasattr(self, '_crawlers'):
            for crawler in self._crawlers.values():
                if hasattr(crawler, 'close'):
                    crawler.close()
            print("ğŸ”’ æ‰€æœ‰çˆ¬è™«å®ä¾‹å·²å…³é—­")


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    # æµ‹è¯•æ•°æ®
    test_data = [
        (1, "æ ¼åŠ›KFR-35GW/(35586)FNhAb-B1(WIFIï¼‰", "ä¸€çº§"),
        (2, "RF12WPdF/NhA-N1JY01(å«ç®¡)", "ä¸€çº§"),
        (3, "æ ¼åŠ›GMV-DH120WL/Dc1", "ä¸€çº§"),
        (4, "å¥¥å…‹æ–¯KFR-26GW/ABC123", "äºŒçº§"),
        (5, "ç¾çš„KFR-35GW/DEF456", "ä¸€çº§")
    ]
    
    processor = FastBatchProcessor(max_workers=2, request_interval=1.5)
    
    try:
        results = processor.process_batch(test_data)
        
        print(f"\nğŸ“‹ å¤„ç†ç»“æœ:")
        for result in results:
            print(f"  {result['original_model']}: {result['validation_result']} - {result['details']}")
            
    finally:
        processor.close()
